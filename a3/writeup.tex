\documentclass[letterpaper,10pt,fleqn]{article}

%example of setting the fleqn parameter to the article class -- the below sets the offset from flush left (fl)
\setlength{\mathindent}{1cm}

\usepackage{graphicx}                                        

\usepackage{amssymb}                                         
\usepackage{amsmath}                                         
\usepackage{amsthm}                                          

\usepackage{alltt}                                           
\usepackage{float}
\usepackage{color}

\usepackage{balance}
\usepackage[TABBOTCAP, tight]{subfigure}
\usepackage{enumitem}

\usepackage{pstricks, pst-node}

%the following sets the geometry of the page
\usepackage{geometry}
\geometry{textheight=9in, textwidth=6.5in}

% random comment

\newcommand{\cred}[1]{{\color{red}#1}}
\newcommand{\cblue}[1]{{\color{blue}#1}}

\usepackage{hyperref}

\usepackage{textcomp}
\usepackage{listings}

\usepackage{wasysym}

\def\name{Sean Rettig}

%% The following metadata will show up in the PDF properties
\hypersetup{
  colorlinks = true,
  urlcolor = black,
  pdfauthor = {\name},
  pdfkeywords = {cs311 ``operating systems''},
  pdftitle = {CS 311 Project},
  pdfsubject = {CS 311 Project},
  pdfpagemode = UseNone
}

\parindent = 0.0 in
\parskip = 0.2 in

\pagestyle{empty}

\numberwithin{equation}{section}

\newcommand{\D}{\mathrm{d}}

\newcommand\invisiblesection[1]{%
  \refstepcounter{section}%
  \addcontentsline{toc}{section}{\protect\numberline{\thesection}#1}%
  \sectionmark{#1}}

\begin{document}

%to remove page numbers, set the page style to empty

\section*{Assignment 3 Write-up}
\hrule

\subsection*{Design}
\begin{itemize}
    \item The 3 different types of processes will be called "reader", "scorer", and "combiner", with the main process eventually becoming the combiner after parsing the arguments, creating the pipes, and forking off the children.
    \item The words and their counts will be stored in a [hash]map.  Update: For the search program, I ended up not only storing the words and counts in hashmaps, but also the document objects themselves, which contained the document path, it's relevance score, and the pointer to its words/counts hashmap.
    \item To implement the maximum of 8 tfidf processes at once, the combiner will first spawn 8, then spawn another one each time it gets an EOF from a tfidf process's pipe (and waits for it) as long as there are still additional files to search.  Update: Due to the tfidf equation requiring the entire database to have already been read to calculate, and the subsequent equation being so small, concurrency for the search program was not implemented both due to time constraints and because it is wholly unnecessary and would most likely only serve to slow down the program significantly.
\end{itemize}

\subsection*{Work Log}
\begin{verbatim}

\end{verbatim}

\subsection*{Challenges Overcame}
\begin{itemize}
    \item Learning how to use the uthash implementation was difficult at first, but definitely worth it given how much time it saved me than if I had to write one my own.
    \item Early on, I had a major issue where my score program was hanging, and I couldn't figure out why.  I finally realized that it was because I hadn't closed all the pipes in every process.
\end{itemize}

\subsection*{Questions}
\begin{enumerate}
    \item The main point of the assignment was to learn the basics of managing multiple processes in Linux, using functions such as fork(), exec(), and wait().  The secondary purposes of this assignment were to learn the semantics of using pipe and stream I/O in Linux, as well as how to handle signals via signal handlers registered through sigaction().
    \item I tested my score program using a variety of file amounts, file sizes, and scorer process amounts, and tested my search program using a variety of different database states, search terms, and search term amounts.  The search program appears to work flawlessly, reading databases containing data for multiple documents and combining the tfidf scores for each search term in each document to produce overall relevance scores for each document.  The scorer, however, only seems to work with one file at a time, for reasons which are unclear to me.
    \item I learned a lot about the uthash hashmap implementation, which I used for both my score and search programs.  I also got more comfortable with pipe/stream I/O and signal handling, but besides that, I don't really feel like I learned much in particular.  Rather, I feel like my C programming skills have improved overall.
\end{enumerate}

\end{document}
